{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setup for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup code for this notebook\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic gto make matplotlib figures appear inline\n",
    "# in the notebook rather than in a new window\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the cifar-10 dataset for Python\n",
    "\n",
    "The following function loads the data inside each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = pickle.load(f, encoding=\"latin1\")\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)    \n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (3073, 49000)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (3073, 1000)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (3073, 10000)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_val=1000, num_test=10000, show_sample=True):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset, and divide the sample into training set, validation set and test set\n",
    "    \"\"\"\n",
    "\n",
    "    cifar10_dir = '../T2/cifar/'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # subsample the data for validation set\n",
    "    mask = range(num_training, num_training + num_val)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def visualize_sample(X_train, y_train, classes, samples_per_class=7):\n",
    "    \"\"\"visualize some samples in the training datasets \"\"\"\n",
    "    num_classes = len(classes)\n",
    "    for y, cls in enumerate(classes):\n",
    "        idxs = np.flatnonzero(y_train == y) # get all the indexes of cls\n",
    "        idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "        for i, idx in enumerate(idxs): # plot the image one by one\n",
    "            plt_idx = i * num_classes + y + 1 # i*num_classes and y+1 determine the row and column respectively\n",
    "            plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "            plt.imshow(X_train[idx].astype('uint8'))\n",
    "            plt.axis('off')\n",
    "            if i == 0:\n",
    "                plt.title(cls)\n",
    "    plt.show()\n",
    "    \n",
    "def preprocessing_CIFAR10_data(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1)) # [49000, 3072]\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1)) # [1000, 3072]\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1)) # [10000, 3072]\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    \n",
    "    # Add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above functions to get our data\n",
    "X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw = get_CIFAR10_data()\n",
    "#visualize_sample(X_train_raw, y_train_raw, classes)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessing_CIFAR10_data(X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw)\n",
    "\n",
    "# As a sanity check, we print out th size of the training and test data dimenstion\n",
    "print ('Train data shape: ', X_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Validation data shape: ', X_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('Test data shape: ', X_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function to apply logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_grad_logistic_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients using logistic function \n",
    "    with loop, which is slow.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W: (1, D) array of weights, D is the dimension of one sample.\n",
    "    X: (D, N) array of training data, each column is a training sample with D-dimension.\n",
    "    y: (N, ) 1-dimension array of target data with length N.\n",
    "    reg: (float) regularization strength for optimization.\n",
    "    Returns\n",
    "    -------\n",
    "    a tuple of two items (loss, grad)\n",
    "    loss: (float)\n",
    "    grad: (array) with respect to self.W\n",
    "    \"\"\"\n",
    "    dim, num_train = X.shape\n",
    "    loss = 0\n",
    "    grad = np.zeros_like(W) # [1, D]\n",
    "    for i in range(num_train):\n",
    "        sample_x = X[:, i]\n",
    "        f_x = 0\n",
    "        for idx in range(sample_x.shape[0]):\n",
    "            f_x += W[0, idx] * sample_x[idx]\n",
    "        h_x = 1.0 / (1 + np.exp(-f_x))\n",
    "        loss += y[i] * np.log(h_x) + (1 - y[i]) * np.log(1 - h_x)\n",
    "\n",
    "        grad += (h_x - y[i]) * sample_x # [D, ]\n",
    "    loss /= -num_train\n",
    "    loss += 0.5 * reg * np.sum(W * W) # add regularization\n",
    "\n",
    "    grad /= num_train\n",
    "    grad += reg * W # add regularization\n",
    "    return loss, grad\n",
    "\n",
    "def loss_grad_logistic_vectorized(W, X, y, reg):\n",
    "    \"\"\"Compute the loss and gradients with weights, vectorized version\"\"\"\n",
    "    dim, num_train = X.shape\n",
    "    loss = 0\n",
    "    grad = np.zeros_like(W) # [1, D]\n",
    "    # print W\n",
    "    f_x_mat = W.dot(X) # [1, D] * [D, N]\n",
    "    h_x_mat = 1.0 / (1.0 + np.exp(-f_x_mat)) # [1, N]\n",
    "    loss = np.sum(y * np.log(h_x_mat) + (1 - y) * np.log(1 - h_x_mat))\n",
    "    loss = -1.0 / num_train * loss + 0.5 * reg * np.sum(W * W)\n",
    "    grad = (h_x_mat - y).dot(X.T) # [1, D]\n",
    "    grad = 1.0 / num_train * grad + reg * W\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: numpy boolean negative, the `-` operator, is deprecated, use the `~` operator or the logical_not function instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 1.351627, and gradient: computed in 87.352677s\n",
      "Vectorized loss: 1.351627, and gradient: computed in 0.405069s\n",
      "[2043 2257 2576 1344 2353 1882  882 2627  726 2944]\n",
      "[-1.51561506 -3.49564526 -4.25881283 -6.13261359 -4.4592427  -4.9802744\n",
      " -4.06111598 -6.33426121 -7.06299065 -7.12707014]\n",
      "[-1.51561506 -3.49564526 -4.25881283 -6.13261359 -4.4592427  -4.9802744\n",
      " -4.06111598 -6.33426121 -7.06299065 -7.12707014]\n",
      "Gradient difference between naive and vectorized version is: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Set the label of the first class to be one, and 0 for others\n",
    "from copy import deepcopy\n",
    "y_train_test_loss = deepcopy(y_train)\n",
    "idxs_zero = y_train_test_loss == 0\n",
    "y_train_test_loss[idxs_zero] = 1\n",
    "y_train_test_loss[-idxs_zero] = 0\n",
    "\n",
    "# Test the loss and gradient and compare between two implementations\n",
    "import time\n",
    "\n",
    "# generate a rand weights W \n",
    "W = np.random.randn(1, X_train.shape[0]) * 0.001\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = loss_grad_logistic_naive(W, X_train, y_train_test_loss, 0)\n",
    "toc = time.time()\n",
    "print (\"Naive loss: %f, and gradient: computed in %fs\" % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vec, grad_vect = loss_grad_logistic_vectorized(W, X_train, y_train_test_loss, 0)\n",
    "toc = time.time()\n",
    "print (\"Vectorized loss: %f, and gradient: computed in %fs\" % (loss_vec, toc - tic))\n",
    "\n",
    "# Compare the gradient, because the gradient is a vector, we canuse the Frobenius norm to compare them\n",
    "# the Frobenius norm of two matrices is the square root of the squared sum of differences of all elements\n",
    "diff = np.linalg.norm(grad_naive - grad_vect, ord='fro')\n",
    "# Randomly choose some gradient to check\n",
    "idxs = np.random.choice(X_train.shape[0], 10, replace=False)\n",
    "print (idxs)\n",
    "print (grad_naive[0, idxs])\n",
    "print (grad_vect[0, idxs])\n",
    "print (\"Gradient difference between naive and vectorized version is: %f\" % diff)\n",
    "del loss_naive, loss_vec, grad_naive, y_train_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3073)\n",
      "(0, 1138)\n",
      "numerical: -2.717364 analytic: -9.088938, relative error: 5.396757e-01\n",
      "(0, 447)\n",
      "numerical: -14.309956 analytic: -9.255200, relative error: 2.145013e-01\n",
      "(0, 2954)\n",
      "numerical: 13.763395 analytic: -5.862279, relative error: 1.000000e+00\n",
      "(0, 530)\n",
      "numerical: -8.317170 analytic: -9.367881, relative error: 5.941242e-02\n",
      "(0, 2704)\n",
      "numerical: 6.792732 analytic: -8.951673, relative error: 1.000000e+00\n",
      "(0, 2548)\n",
      "numerical: 10.437505 analytic: -6.551486, relative error: 1.000000e+00\n",
      "(0, 2685)\n",
      "numerical: 3.598830 analytic: -5.264559, relative error: 1.000000e+00\n",
      "(0, 407)\n",
      "numerical: -12.456615 analytic: -7.099066, relative error: 2.739638e-01\n",
      "(0, 2647)\n",
      "numerical: 10.920202 analytic: -6.884329, relative error: 1.000000e+00\n",
      "(0, 2755)\n",
      "numerical: 11.858943 analytic: -6.832603, relative error: 1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# file: algorithms/gradient_check.py\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks):\n",
    "  \"\"\"\n",
    "  sample a few random elements and only return numerical\n",
    "  in this dimensions.\n",
    "  \"\"\"\n",
    "  h = 1e-5\n",
    "\n",
    "  print (x.shape)\n",
    "\n",
    "  for i in range(num_checks):\n",
    "    ix = tuple([random.randrange(m) for m in x.shape])\n",
    "    print (ix)\n",
    "    x[ix] += h # increment by h\n",
    "    fxph = f(x) # evaluate f(x + h)\n",
    "    x[ix] -= 2 * h # increment by h\n",
    "    fxmh = f(x) # evaluate f(x - h)\n",
    "    x[ix] += h # reset\n",
    "\n",
    "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "    grad_analytic = analytic_grad[ix]\n",
    "    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "    print (\"numerical: %f analytic: %f, relative error: %e\" % (grad_numerical, grad_analytic, rel_error))\n",
    "    \n",
    "# Check gradient using numerical gradient along several randomly chosen dimenstion\n",
    "\n",
    "f = lambda w: loss_grad_logistic_vectorized(w, X_train, y_train, 0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad_vect, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W = None # set up the weight matrix \n",
    "\n",
    "    def train(self, X, y, method='sgd', batch_size=200, learning_rate=1e-4,\n",
    "              reg = 1e3, num_iters=1000, verbose=False, vectorized=True):\n",
    "        \"\"\"\n",
    "        Train linear classifer using batch gradient descent or stochastic gradient descent\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: (D x N) array of training data, each column is a training sample with D-dimension.\n",
    "        y: (N, ) 1-dimension array of target data with length N.\n",
    "        method: (string) determine whether using 'bgd' or 'sgd'.\n",
    "        batch_size: (integer) number of training examples to use at each step.\n",
    "        learning_rate: (float) learning rate for optimization.\n",
    "        reg: (float) regularization strength for optimization.\n",
    "        num_iters: (integer) number of steps to take when optimization.\n",
    "        verbose: (boolean) if True, print out the progress (loss) when optimization.\n",
    "        Returns\n",
    "        -------\n",
    "        losses_history: (list) of losses at each training iteration\n",
    "        \"\"\"\n",
    "\n",
    "        dim, num_train = X.shape\n",
    "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "\n",
    "        if self.W is None:\n",
    "            # initialize the weights with small values\n",
    "            if num_classes == 2: # just need weights for one class\n",
    "                self.W = np.random.randn(1, dim) * 0.001\n",
    "            else: # weigths for each class\n",
    "                self.W = np.random.randn(num_classes, dim) * 0.001\n",
    "\n",
    "        losses_history = []\n",
    "\n",
    "        for i in range(num_iters):\n",
    "            if method == 'bgd':\n",
    "                loss, grad = self.loss_grad(X, y, reg, vectorized)\n",
    "            else:\n",
    "                # randomly choose a min-batch of samples\n",
    "                idxs = np.random.choice(num_train, batch_size, replace=False)\n",
    "                loss, grad = self.loss_grad(X[:, idxs], y[idxs], reg, vectorized) # grad => [K x D]\n",
    "            losses_history.append(loss)\n",
    "\n",
    "            # update weights\n",
    "            self.W -= learning_rate * grad # [K x D]\n",
    "            # print self.W\n",
    "            # print 'dsfad', grad.shape\n",
    "            if verbose and (i % 100 == 0):\n",
    "                print (\"iteration %d/%d: loss %f\" % (i, num_iters, loss))\n",
    "\n",
    "        return losses_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict value of y using trained weights\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: (D x N) array of data, each column is a sample with D-dimension.\n",
    "        Returns\n",
    "        -------\n",
    "        pred_ys: (N, ) 1-dimension array of y for N sampels\n",
    "        h_x_mat: Normalized scores\n",
    "        \"\"\"\n",
    "        pred_ys = np.zeros(X.shape[1])\n",
    "        f_x_mat = self.W.dot(X)\n",
    "        if self.__class__.__name__ == 'Logistic':\n",
    "            pred_ys = f_x_mat.squeeze() >=0\n",
    "        else: # multiclassification\n",
    "            pred_ys = np.argmax(f_x_mat, axis=0)\n",
    "        # normalized score\n",
    "        h_x_mat = 1.0 / (1.0 + np.exp(-f_x_mat)) # [1, N]\n",
    "        h_x_mat = h_x_mat.squeeze()\n",
    "        return pred_ys, h_x_mat\n",
    "\n",
    "    def loss_grad(self, X, y, reg, vectorized=True):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients.\n",
    "        Parameters\n",
    "        ----------\n",
    "        The same as self.train()\n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of two items (loss, grad)\n",
    "        loss: (float)\n",
    "        grad: (array) with respect to self.W\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# Subclasses of linear classifier\n",
    "class Logistic(LinearClassifier):\n",
    "    \"\"\"A subclass for binary classification using logistic function\"\"\"\n",
    "    def loss_grad(self, X, y, reg, vectorized=True):\n",
    "        if vectorized:\n",
    "            return loss_grad_logistic_vectorized(self.W, X, y, reg)\n",
    "        else:\n",
    "            return loss_grad_logistic_naive(self.W, X, y, reg)\n",
    "\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "    \"\"\"A subclass for multi-classicication using Softmax function\"\"\"\n",
    "    def loss_grad(self, X, y, reg, vectorized=True):\n",
    "        if vectorized:\n",
    "            return loss_grad_softmax_vectorized(self.W, X, y, reg)\n",
    "        else:\n",
    "            return loss_grad_softmax_naive(self.W, X, y, reg)\n",
    "\n",
    "\n",
    "class SVM(LinearClassifier):\n",
    "    \"\"\"A subclass for multi-classicication using SVM function\"\"\"\n",
    "    def loss_grad(self, X, y, reg, vectorized=True):\n",
    "        return loss_grad_svm_vectorized(self.W, X, y, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 1/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.682308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: numpy boolean negative, the `-` operator, is deprecated, use the `~` operator or the logical_not function instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100/1000: loss 1.906143\n",
      "iteration 200/1000: loss 1.726622\n",
      "iteration 300/1000: loss 1.494798\n",
      "iteration 400/1000: loss 1.309114\n",
      "iteration 500/1000: loss 1.201118\n",
      "iteration 600/1000: loss 1.164320\n",
      "iteration 700/1000: loss 1.047020\n",
      "iteration 800/1000: loss 0.964222\n",
      "iteration 900/1000: loss 0.896151\n",
      "\n",
      "The 2/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.947436\n",
      "iteration 100/1000: loss 2.096683\n",
      "iteration 200/1000: loss 1.776123\n",
      "iteration 300/1000: loss 1.551808\n",
      "iteration 400/1000: loss 1.379287\n",
      "iteration 500/1000: loss 1.160780\n",
      "iteration 600/1000: loss 1.126292\n",
      "iteration 700/1000: loss 0.998693\n",
      "iteration 800/1000: loss 1.000904\n",
      "iteration 900/1000: loss 0.907249\n",
      "\n",
      "The 3/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.236797\n",
      "iteration 100/1000: loss 2.056151\n",
      "iteration 200/1000: loss 1.848849\n",
      "iteration 300/1000: loss 1.522873\n",
      "iteration 400/1000: loss 1.363199\n",
      "iteration 500/1000: loss 1.250484\n",
      "iteration 600/1000: loss 1.156171\n",
      "iteration 700/1000: loss 1.017332\n",
      "iteration 800/1000: loss 0.984024\n",
      "iteration 900/1000: loss 0.944673\n",
      "\n",
      "The 4/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.088229\n",
      "iteration 100/1000: loss 2.002459\n",
      "iteration 200/1000: loss 1.731528\n",
      "iteration 300/1000: loss 1.502362\n",
      "iteration 400/1000: loss 1.365735\n",
      "iteration 500/1000: loss 1.219197\n",
      "iteration 600/1000: loss 1.097864\n",
      "iteration 700/1000: loss 1.079084\n",
      "iteration 800/1000: loss 0.994956\n",
      "iteration 900/1000: loss 0.904463\n",
      "\n",
      "The 5/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.827117\n",
      "iteration 100/1000: loss 2.073701\n",
      "iteration 200/1000: loss 1.735510\n",
      "iteration 300/1000: loss 1.546460\n",
      "iteration 400/1000: loss 1.427489\n",
      "iteration 500/1000: loss 1.233294\n",
      "iteration 600/1000: loss 1.178385\n",
      "iteration 700/1000: loss 1.054218\n",
      "iteration 800/1000: loss 0.953435\n",
      "iteration 900/1000: loss 0.910471\n",
      "\n",
      "The 6/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.689127\n",
      "iteration 100/1000: loss 1.966236\n",
      "iteration 200/1000: loss 1.707662\n",
      "iteration 300/1000: loss 1.540084\n",
      "iteration 400/1000: loss 1.326390\n",
      "iteration 500/1000: loss 1.214232\n",
      "iteration 600/1000: loss 1.091835\n",
      "iteration 700/1000: loss 1.014354\n",
      "iteration 800/1000: loss 0.937995\n",
      "iteration 900/1000: loss 0.914094\n",
      "\n",
      "The 7/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.035341\n",
      "iteration 100/1000: loss 1.984937\n",
      "iteration 200/1000: loss 1.752643\n",
      "iteration 300/1000: loss 1.545471\n",
      "iteration 400/1000: loss 1.365025\n",
      "iteration 500/1000: loss 1.198290\n",
      "iteration 600/1000: loss 1.110720\n",
      "iteration 700/1000: loss 1.029424\n",
      "iteration 800/1000: loss 0.953299\n",
      "iteration 900/1000: loss 0.885101\n",
      "\n",
      "The 8/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.794093\n",
      "iteration 100/1000: loss 2.014622\n",
      "iteration 200/1000: loss 1.745645\n",
      "iteration 300/1000: loss 1.515817\n",
      "iteration 400/1000: loss 1.341659\n",
      "iteration 500/1000: loss 1.214623\n",
      "iteration 600/1000: loss 1.070539\n",
      "iteration 700/1000: loss 1.037423\n",
      "iteration 800/1000: loss 0.961487\n",
      "iteration 900/1000: loss 0.912328\n",
      "\n",
      "The 9/10th logistic classifier training...\n",
      "iteration 0/1000: loss 2.871148\n",
      "iteration 100/1000: loss 2.050370\n",
      "iteration 200/1000: loss 1.723897\n",
      "iteration 300/1000: loss 1.510706\n",
      "iteration 400/1000: loss 1.347605\n",
      "iteration 500/1000: loss 1.183102\n",
      "iteration 600/1000: loss 1.126535\n",
      "iteration 700/1000: loss 1.017538\n",
      "iteration 800/1000: loss 0.930238\n",
      "iteration 900/1000: loss 0.894184\n",
      "\n",
      "The 10/10th logistic classifier training...\n",
      "iteration 0/1000: loss 3.109384\n",
      "iteration 100/1000: loss 2.153841\n",
      "iteration 200/1000: loss 1.833757\n",
      "iteration 300/1000: loss 1.544006\n",
      "iteration 400/1000: loss 1.394980\n",
      "iteration 500/1000: loss 1.271981\n",
      "iteration 600/1000: loss 1.117757\n",
      "iteration 700/1000: loss 1.010646\n",
      "iteration 800/1000: loss 0.971814\n",
      "iteration 900/1000: loss 0.911821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logistic_classifiers = []\n",
    "num_classes = np.max(y_train) + 1\n",
    "losses = []\n",
    "for i in range(num_classes):\n",
    "    print (\"\\nThe %d/%dth logistic classifier training...\" % (i+1, num_classes))\n",
    "    y_train_logistic = deepcopy(y_train)\n",
    "    idxs_i = y_train_logistic == i\n",
    "    y_train_logistic[idxs_i] = 1\n",
    "    y_train_logistic[-idxs_i] = 0\n",
    "    logistic = Logistic()\n",
    "    loss = logistic.train(X_train, y_train_logistic, method='sgd', batch_size=200, learning_rate=1e-6,\n",
    "              reg = 1e3, num_iters=1000, verbose=True, vectorized=True)\n",
    "    losses.append(loss)\n",
    "    logistic_classifiers.append(logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ggplot\\components\\smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:913: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp8AAAHmCAYAAAA1LeWcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23d4e8c3a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (153895344048)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ggplot import *\n",
    "# plot the loss of the last trained logistic classifier\n",
    "qplot(range(len(losses[9])), losses[9]) + labs(x='Iteration number', y='SGD Loss for last trained classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset accuracy: 0.346918\n",
      "Validation dataset accuracy: 0.353000\n",
      "Test datast accuracy: 0.338100\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of training data and validation data\n",
    "def predict_one_vs_all(logistic_classifiers, X, num_classes):\n",
    "    scores = np.zeros((num_classes, X.shape[1]))\n",
    "    for i in range(num_classes):\n",
    "        logistic = logistic_classifiers[i]\n",
    "        scores[i, :] = logistic.predict(X)[1]\n",
    "    pred_X = np.argmax(scores, axis=0)\n",
    "    return pred_X\n",
    "\n",
    "pred_train_one_vs_all = predict_one_vs_all(logistic_classifiers, X_train, num_classes)\n",
    "pred_val_one_vs_all = predict_one_vs_all(logistic_classifiers, X_val, num_classes)\n",
    "pred_test_one_vs_all = predict_one_vs_all(logistic_classifiers, X_test, num_classes)\n",
    "print (\"Training dataset accuracy: %f\" % (np.mean(y_train == pred_train_one_vs_all)))\n",
    "print (\"Validation dataset accuracy: %f\" % (np.mean(y_val == pred_val_one_vs_all)))\n",
    "print (\"Test datast accuracy: %f\" % (np.mean(y_test == pred_test_one_vs_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file: algorithms/classifiers/loss_grad_softmax.py\n",
    "import numpy as np\n",
    "\n",
    "def loss_grad_softmax_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients using softmax function \n",
    "    with loop, which is slow.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W: (K, D) array of weights, K is the number of classes and D is the dimension of one sample.\n",
    "    X: (D, N) array of training data, each column is a training sample with D-dimension.\n",
    "    y: (N, ) 1-dimension array of target data with length N with lables 0,1, ... K-1, for K classes\n",
    "    reg: (float) regularization strength for optimization.\n",
    "    Returns\n",
    "    -------\n",
    "    a tuple of two items (loss, grad)\n",
    "    loss: (float)\n",
    "    grad: (K, D) with respect to W\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    grad = np.zeros_like(W)\n",
    "    dim, num_train = X.shape\n",
    "    num_classes = W.shape[0]\n",
    "    for i in range(num_train):\n",
    "        sample_x = X[:, i]\n",
    "        scores = np.zeros(num_classes) # [K, 1] unnormalized score\n",
    "        for cls in range(num_classes):\n",
    "            w = W[cls, :]\n",
    "            scores[cls] = w.dot(sample_x)\n",
    "        # Shift the scores so that the highest value is 0\n",
    "        scores -= np.max(scores)\n",
    "        correct_class = y[i]\n",
    "        sum_exp_scores = np.sum(np.exp(scores))\n",
    "\n",
    "        corr_cls_exp_score = np.exp(scores[correct_class])\n",
    "        loss_x = -np.log(corr_cls_exp_score / sum_exp_scores)\n",
    "        loss += loss_x\n",
    "\n",
    "        # compute the gradient\n",
    "        percent_exp_score = np.exp(scores) / sum_exp_scores\n",
    "        for j in range(num_classes):\n",
    "            grad[j, :] += percent_exp_score[j] * sample_x\n",
    "\n",
    "\n",
    "        grad[correct_class, :] -= sample_x # deal with the correct class\n",
    "\n",
    "    loss /= num_train\n",
    "    loss += 0.5 * reg * np.sum(W * W) # add regularization\n",
    "    grad /= num_train\n",
    "    grad += reg * W\n",
    "    return loss, grad\n",
    "\n",
    "def loss_grad_softmax_vectorized(W, X, y, reg):\n",
    "    \"\"\" Compute the loss and gradients using softmax with vectorized version\"\"\"\n",
    "    loss = 0 \n",
    "    grad = np.zeros_like(W)\n",
    "    dim, num_train = X.shape\n",
    "\n",
    "    scores = W.dot(X) # [K, N]\n",
    "    # Shift scores so that the highest value is 0\n",
    "    scores -= np.max(scores)\n",
    "    scores_exp = np.exp(scores)\n",
    "    correct_scores_exp = scores_exp[y, range(num_train)] # [N, ]\n",
    "    scores_exp_sum = np.sum(scores_exp, axis=0) # [N, ]\n",
    "    loss = -np.sum(np.log(correct_scores_exp / scores_exp_sum))\n",
    "    loss /= num_train\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "\n",
    "    scores_exp_normalized = scores_exp / scores_exp_sum\n",
    "    # deal with the correct class\n",
    "    scores_exp_normalized[y, range(num_train)] -= 1 # [K, N]\n",
    "    grad = scores_exp_normalized.dot(X.T)\n",
    "    grad /= num_train\n",
    "    grad += reg * W\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 6.178285, and gradient: computed in 8.651006s\n",
      "Vectorized loss: 6.178285, and gradient: computed in 0.655744s\n",
      "[1829  497 2432 2870  769  242  325 2999 1394  585]\n",
      "[-5.17753048 -3.51010405 -3.35234834 -4.92026973 -3.81397567 -3.77778575\n",
      " -2.68507335 -5.72808938 -2.30037019 -2.3766423 ]\n",
      "[-5.17753048 -3.51010405 -3.35234834 -4.92026973 -3.81397567 -3.77778575\n",
      " -2.68507335 -5.72808938 -2.30037019 -2.3766423 ]\n",
      "Gradient difference between naive and vectorized version is: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# generate a rand weights W \n",
    "W = np.random.randn(10, X_train.shape[0]) * 0.001\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = loss_grad_softmax_naive(W, X_train, y_train, 0.0001)\n",
    "toc = time.time()\n",
    "print (\"Naive loss: %f, and gradient: computed in %fs\" % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vec, grad_vect = loss_grad_softmax_vectorized(W, X_train, y_train, 0.0001)\n",
    "toc = time.time()\n",
    "print (\"Vectorized loss: %f, and gradient: computed in %fs\" % (loss_vec, toc - tic))\n",
    "\n",
    "# Compare the gradient, because the gradient is a vector, we canuse the Frobenius norm to compare them\n",
    "# the Frobenius norm of two matrices is the square root of the squared sum of differences of all elements\n",
    "diff = np.linalg.norm(grad_naive - grad_vect, ord='fro')\n",
    "# Randomly choose some gradient to check\n",
    "idxs = np.random.choice(X_train.shape[0], 10, replace=False)\n",
    "print (idxs)\n",
    "print (grad_naive[0, idxs])\n",
    "print (grad_vect[0, idxs])\n",
    "print (\"Gradient difference between naive and vectorized version is: %f\" % diff)\n",
    "del loss_naive, loss_vec, grad_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_check_sparse(f, x, analytic_grad, num_checks):\n",
    "  \"\"\"\n",
    "  sample a few random elements and only return numerical\n",
    "  in this dimensions.\n",
    "  \"\"\"\n",
    "  h = 1e-5\n",
    "\n",
    "  print (x.shape)\n",
    "\n",
    "  for i in range(num_checks):\n",
    "    ix = tuple([random.randrange(m) for m in x.shape])\n",
    "    print (ix)\n",
    "    x[ix] += h # increment by h\n",
    "    fxph = f(x) # evaluate f(x + h)\n",
    "    x[ix] -= 2 * h # increment by h\n",
    "    fxmh = f(x) # evaluate f(x - h)\n",
    "    x[ix] += h # reset\n",
    "\n",
    "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "    grad_analytic = analytic_grad[ix]\n",
    "    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "    print (\"numerical: %f analytic: %f, relative error: %e\" % (grad_numerical, grad_analytic, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3073)\n",
      "(9, 1371)\n",
      "numerical: 0.465232 analytic: 0.465232, relative error: 2.231595e-07\n",
      "(7, 547)\n",
      "numerical: -0.567056 analytic: -0.567056, relative error: 1.939800e-08\n",
      "(5, 226)\n",
      "numerical: 0.782446 analytic: 0.782446, relative error: 2.011063e-08\n",
      "(3, 2889)\n",
      "numerical: -4.014020 analytic: -4.014020, relative error: 6.264119e-09\n",
      "(1, 434)\n",
      "numerical: 1.145204 analytic: 1.145204, relative error: 6.263633e-08\n",
      "(7, 687)\n",
      "numerical: -0.034106 analytic: -0.034106, relative error: 2.537701e-06\n",
      "(6, 906)\n",
      "numerical: 1.798075 analytic: 1.798075, relative error: 1.248726e-08\n",
      "(2, 217)\n",
      "numerical: 10.985031 analytic: 10.985030, relative error: 9.748595e-10\n",
      "(5, 774)\n",
      "numerical: 1.216765 analytic: 1.216766, relative error: 1.893262e-08\n",
      "(3, 1387)\n",
      "numerical: -0.548897 analytic: -0.548897, relative error: 9.124345e-08\n"
     ]
    }
   ],
   "source": [
    "# Check gradient using numerical gradient along several randomly chosen dimenstion\n",
    "f = lambda w: loss_grad_softmax_vectorized(w, X_train, y_train, 0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad_vect, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0/1000: loss 1554.820469\n",
      "iteration 100/1000: loss 2.125524\n",
      "iteration 200/1000: loss 2.178786\n",
      "iteration 300/1000: loss 2.128934\n",
      "iteration 400/1000: loss 2.194947\n",
      "iteration 500/1000: loss 2.087499\n",
      "iteration 600/1000: loss 2.115973\n",
      "iteration 700/1000: loss 2.137301\n",
      "iteration 800/1000: loss 2.122270\n",
      "iteration 900/1000: loss 2.161467\n",
      "Traning time for SGD with vectorized version is 8.050409 \n",
      "\n",
      "Training accuracy: 0.291714\n",
      "Validation accuracy: 0.310000\n"
     ]
    }
   ],
   "source": [
    "# # using BGD algorithm\n",
    "# softmax_bgd = Softmax()\n",
    "# tic = time.time()\n",
    "# losses_bgd = softmax_bgd.train(X_train, y_train, method='bgd', batch_size=200, learning_rate=1e-6,\n",
    "#               reg = 1e2, num_iters=1000, verbose=True, vectorized=True)\n",
    "# toc = time.time()\n",
    "# print 'Traning time for BGD with vectorized version is %f \\n' % (toc - tic)\n",
    "\n",
    "# # Compute the accuracy of training data and validation data using Softmax.predict function\n",
    "# y_train_pred_bgd = softmax_bgd.predict(X_train)[0]\n",
    "# print 'Training accuracy: %f' % (np.mean(y_train == y_train_pred_bgd))\n",
    "# y_val_pred_bgd = softmax_bgd.predict(X_val)[0]\n",
    "# print 'Validation accuracy: %f' % (np.mean(y_val == y_val_pred_bgd))\n",
    "\n",
    "# # using SGD algorithm\n",
    "softmax_sgd = Softmax()\n",
    "tic = time.time()\n",
    "losses_sgd = softmax_sgd.train(X_train, y_train, method='sgd', batch_size=200, learning_rate=1e-6,\n",
    "              reg = 1e5, num_iters=1000, verbose=True, vectorized=True)\n",
    "toc = time.time()\n",
    "print (\"Traning time for SGD with vectorized version is %f \\n\" % (toc - tic))\n",
    "\n",
    "y_train_pred_sgd = softmax_sgd.predict(X_train)[0]\n",
    "print (\"Training accuracy: %f\" % (np.mean(y_train == y_train_pred_sgd)))\n",
    "y_val_pred_sgd = softmax_sgd.predict(X_val)[0]\n",
    "print (\"Validation accuracy: %f\" % (np.mean(y_val == y_val_pred_sgd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:913: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqoAAAHmCAYAAAC796zgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23d4e5bb978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (153895687870)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ggplot import *\n",
    "qplot(range(len(losses_sgd)), losses_sgd) + labs(x='Iteration number', y='SGD Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current iteration is 1/5\n",
      "The current iteration is 2/5\n",
      "The current iteration is 3/5\n",
      "The current iteration is 4/5\n",
      "The current iteration is 5/5\n",
      "learning rate 1.000000e-08 and regularization 1.000000e+03, \n",
      "     the training accuracy is: 0.160408 and validation accuracy is: 0.160000.\n",
      "\n",
      "learning rate 1.000000e-08 and regularization 2.575000e+04, \n",
      "     the training accuracy is: 0.137469 and validation accuracy is: 0.134000.\n",
      "\n",
      "learning rate 1.000000e-08 and regularization 5.050000e+04, \n",
      "     the training accuracy is: 0.138429 and validation accuracy is: 0.130000.\n",
      "\n",
      "learning rate 1.000000e-08 and regularization 7.525000e+04, \n",
      "     the training accuracy is: 0.160041 and validation accuracy is: 0.143000.\n",
      "\n",
      "learning rate 1.000000e-08 and regularization 1.000000e+05, \n",
      "     the training accuracy is: 0.166367 and validation accuracy is: 0.172000.\n",
      "\n",
      "learning rate 2.507500e-06 and regularization 1.000000e+03, \n",
      "     the training accuracy is: 0.400204 and validation accuracy is: 0.390000.\n",
      "\n",
      "learning rate 2.507500e-06 and regularization 2.575000e+04, \n",
      "     the training accuracy is: 0.321694 and validation accuracy is: 0.337000.\n",
      "\n",
      "learning rate 2.507500e-06 and regularization 5.050000e+04, \n",
      "     the training accuracy is: 0.290918 and validation accuracy is: 0.290000.\n",
      "\n",
      "learning rate 2.507500e-06 and regularization 7.525000e+04, \n",
      "     the training accuracy is: 0.300184 and validation accuracy is: 0.314000.\n",
      "\n",
      "learning rate 2.507500e-06 and regularization 1.000000e+05, \n",
      "     the training accuracy is: 0.279837 and validation accuracy is: 0.289000.\n",
      "\n",
      "learning rate 5.005000e-06 and regularization 1.000000e+03, \n",
      "     the training accuracy is: 0.362694 and validation accuracy is: 0.331000.\n",
      "\n",
      "learning rate 5.005000e-06 and regularization 2.575000e+04, \n",
      "     the training accuracy is: 0.285959 and validation accuracy is: 0.286000.\n",
      "\n",
      "learning rate 5.005000e-06 and regularization 5.050000e+04, \n",
      "     the training accuracy is: 0.204306 and validation accuracy is: 0.222000.\n",
      "\n",
      "learning rate 5.005000e-06 and regularization 7.525000e+04, \n",
      "     the training accuracy is: 0.158816 and validation accuracy is: 0.164000.\n",
      "\n",
      "learning rate 5.005000e-06 and regularization 1.000000e+05, \n",
      "     the training accuracy is: 0.157857 and validation accuracy is: 0.154000.\n",
      "\n",
      "learning rate 7.502500e-06 and regularization 1.000000e+03, \n",
      "     the training accuracy is: 0.327633 and validation accuracy is: 0.346000.\n",
      "\n",
      "learning rate 7.502500e-06 and regularization 2.575000e+04, \n",
      "     the training accuracy is: 0.195796 and validation accuracy is: 0.201000.\n",
      "\n",
      "learning rate 7.502500e-06 and regularization 5.050000e+04, \n",
      "     the training accuracy is: 0.160653 and validation accuracy is: 0.150000.\n",
      "\n",
      "learning rate 7.502500e-06 and regularization 7.525000e+04, \n",
      "     the training accuracy is: 0.093490 and validation accuracy is: 0.107000.\n",
      "\n",
      "learning rate 7.502500e-06 and regularization 1.000000e+05, \n",
      "     the training accuracy is: 0.078367 and validation accuracy is: 0.072000.\n",
      "\n",
      "learning rate 1.000000e-05 and regularization 1.000000e+03, \n",
      "     the training accuracy is: 0.246653 and validation accuracy is: 0.239000.\n",
      "\n",
      "learning rate 1.000000e-05 and regularization 2.575000e+04, \n",
      "     the training accuracy is: 0.223286 and validation accuracy is: 0.235000.\n",
      "\n",
      "learning rate 1.000000e-05 and regularization 5.050000e+04, \n",
      "     the training accuracy is: 0.092592 and validation accuracy is: 0.071000.\n",
      "\n",
      "learning rate 1.000000e-05 and regularization 7.525000e+04, \n",
      "     the training accuracy is: 0.131204 and validation accuracy is: 0.122000.\n",
      "\n",
      "learning rate 1.000000e-05 and regularization 1.000000e+05, \n",
      "     the training accuracy is: 0.124082 and validation accuracy is: 0.127000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using validation set to tuen hyperparameters, i.e., learning rate and regularization strength\n",
    "learning_rates = [1e-5, 1e-8]\n",
    "regularization_strengths = [10e2, 10e4]\n",
    "\n",
    "# Result is a dictionary mapping tuples of the form (learning_rate, regularization_strength) \n",
    "# to tuples of the form (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "# Choose the best hyperparameters by tuning on the validation set\n",
    "i = 0\n",
    "interval = 5\n",
    "for learning_rate in np.linspace(learning_rates[0], learning_rates[1], num=interval):\n",
    "    i += 1\n",
    "    print (\"The current iteration is %d/%d\" % (i, interval))\n",
    "    for reg in np.linspace(regularization_strengths[0], regularization_strengths[1], num=interval):\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, method='sgd', batch_size=200, learning_rate=learning_rate,\n",
    "              reg = reg, num_iters=1000, verbose=False, vectorized=True)\n",
    "        y_train_pred = softmax.predict(X_train)[0]\n",
    "        y_val_pred = softmax.predict(X_val)[0]\n",
    "        train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[(learning_rate, reg)] = (train_accuracy, val_accuracy)\n",
    "        if val_accuracy > best_val:\n",
    "            best_val = val_accuracy\n",
    "            best_softmax = softmax\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Print out the results\n",
    "for learning_rate, reg in sorted(results):\n",
    "    train_accuracy,val_accuracy = results[(learning_rate, reg)]\n",
    "    print (\"learning rate %e and regularization %e, \\n \\\n",
    "    the training accuracy is: %f and validation accuracy is: %f.\\n\" % (learning_rate, reg, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: 0.392100\n"
     ]
    }
   ],
   "source": [
    "y_test_predict_result = best_softmax.predict(X_test)\n",
    "y_test_predict = y_test_predict_result[0]\n",
    "test_accuracy = np.mean(y_test == y_test_predict)\n",
    "print (\"The test accuracy is: %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "n_inputs = 50000\n",
    "n_hidden1 = 10000\n",
    "n_hidden2 = 1000\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network with 1 hidden layer with 2000 neurons\n",
    "\n",
    "Taking 3500 steps to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023D6A35FDA0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\VICTOR~1\\\\AppData\\\\Local\\\\Temp\\\\tmp2213d54l'}\n",
      "WARNING:tensorflow:From C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt.\n",
      "INFO:tensorflow:loss = 126.723, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 41 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 86 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.070949\n",
      "INFO:tensorflow:loss = 2.30719, step = 101 (1409.636 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 130 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 170 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.069717\n",
      "INFO:tensorflow:loss = 2.31416, step = 201 (1434.336 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 214 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 258 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a29d65577430>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdnn_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNNClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_hidden1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdnn_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSKCompat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdnn_clf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# if TensorFlow >= 1.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdnn_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, steps, max_steps, monitors)\u001b[0m\n\u001b[0;32m   1351\u001b[0m                         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m                         \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1353\u001b[1;33m                         monitors=all_monitors)\n\u001b[0m\u001b[0;32m   1354\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m               instructions)\n\u001b[1;32m--> 296\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    298\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[0;32m    456\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m           \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_fn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m       \u001b[0msummary_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSummaryWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    516\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    860\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    970\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[n_hidden1], n_classes=10, feature_columns=feature_cols, config=config)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1\n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmp2213d54l\\model.ckpt-258\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "tensor_name = dnn/hiddenlayer_0/weights; shape in shape_and_slice spec [10000,10000] does not match the shape stored in checkpoint: [49000,10000]\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_1', defined at:\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-1a86901d1f36>\", line 3, in <module>\n    y_pred = dnn_clf.predict(X_test)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1383, in predict\n    iterate_batches=True))\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 893, in _infer_model\n    config=self._session_config))\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 668, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 490, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 842, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 847, in _create_session\n    return self._sess_creator.create_session()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 551, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 416, in create_session\n    self._scaffold.finalize()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 207, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 733, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 684, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 450, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 663, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): tensor_name = dnn/hiddenlayer_0/weights; shape in shape_and_slice spec [10000,10000] does not match the shape stored in checkpoint: [49000,10000]\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: tensor_name = dnn/hiddenlayer_0/weights; shape in shape_and_slice spec [10000,10000] does not match the shape stored in checkpoint: [49000,10000]\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1a86901d1f36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnn_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3073\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, outputs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m             \u001b[0mas_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m             iterate_batches=True))\n\u001b[0m\u001b[0;32m   1384\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_infer_model\u001b[1;34m(self, input_fn, feed_fn, outputs, as_iterable, iterate_batches)\u001b[0m\n\u001b[0;32m    891\u001b[0m               \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m               \u001b[0mscaffold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfer_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaffold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m               config=self._session_config))\n\u001b[0m\u001b[0;32m    894\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mas_iterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    666\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[0;32m    667\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0;32m    489\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sess_creator)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \"\"\"\n\u001b[0;32m    841\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m     \u001b[0m_WrappedSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         logging.info('An error was raised while a session was being created. '\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    549\u001b[0m       \u001b[1;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m       \u001b[1;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m       \u001b[1;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[1;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m         config=config)\n\u001b[0m\u001b[0;32m    274\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[1;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m       \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_filename_with_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1558\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1560\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: tensor_name = dnn/hiddenlayer_0/weights; shape in shape_and_slice spec [10000,10000] does not match the shape stored in checkpoint: [49000,10000]\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_1', defined at:\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-1a86901d1f36>\", line 3, in <module>\n    y_pred = dnn_clf.predict(X_test)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1383, in predict\n    iterate_batches=True))\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 893, in _infer_model\n    config=self._session_config))\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 668, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 490, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 842, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 847, in _create_session\n    return self._sess_creator.create_session()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 551, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 416, in create_session\n    self._scaffold.finalize()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 207, in finalize\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 733, in _get_saver_or_default\n    saver = Saver(sharded=True, allow_empty=True)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 684, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 450, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 663, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): tensor_name = dnn/hiddenlayer_0/weights; shape in shape_and_slice spec [10000,10000] does not match the shape stored in checkpoint: [49000,10000]\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "y_pred['classes'] = y_pred['classes'].shape(3073,1)\n",
    "accuracy_score(y_test, y_pred['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_pred_proba = y_pred['probabilities']\n",
    "log_loss(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
