{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instructions\n",
    "* One vs all logistic regression\n",
    "* Softmax regression = generalization to handle multiple classes\n",
    "* Neural network with one hidden layer, and numerically checking the gradient\n",
    "* Now 2 hidden layers and different activation f'ns, see what performs best\n",
    "* With best model, do confusion matrix on test set\n",
    "* 4 page report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notebook setup\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Importing the data\n",
    "\n",
    "We have 50 000 32x32 images in the train set and a \"labels\" file with 50 000 lines.\n",
    "\n",
    "For the images, we'll store them as a 50000x3072 np array, so the first 1024 columns are the value of the red pixel in the image of that row, and the next two 1024 columns are the green and blue values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(cifar_dirname=\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\\", upperbound=50000):    \n",
    "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    X = np.empty(shape=(1,3072))\n",
    "    # we load the 50 000 images in X one after the other, one image being 1x3072, to obtain the 50000x3072 array\n",
    "    for i in range(upperbound):\n",
    "        # to have an update of where we're at once in a while\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        # open the current image\n",
    "        current_im  = Image.open(cifar_dirname + str(i).zfill(5) + \".png\" )\n",
    "        # reshape it into an (1,3072) array, so the red values of all the pixels, then green then blue (32*32*3)\n",
    "        reshaped_im = np.reshape(np.asarray(current_im, 'uint8'), (1,3072))\n",
    "        # vertical stack the image into X, what will contain all the images\n",
    "        X           = np.vstack([X, reshaped_im])\n",
    "        \n",
    "    # we want the array to be of type unsigned int on 8 bit (between 0 and 255), so that it occupies the minimum space\n",
    "    X = X.astype(\"uint8\")\n",
    "    X = np.delete(X, (0), axis=0) # delete the first row that's empty\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = get_data()\n",
    "# we load the 50 000 labels for the txt file\n",
    "y = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\labels\")\n",
    "y.shape\n",
    "# since loading the images took a lot of time (~3hr), we'll save them in a binary file (.npy)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now to get the nparray back\n",
    "X = np.load(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images.npy\")\n",
    "# also, for the y array, like earlier:\n",
    "y = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One vs all classifiers\n",
    "\n",
    "We need to do feature scaling on the images before feeding them into the algorithms. We first get the values between -1 and 1: since they're all between 0 and 255, we'll divide by 127 and substract 1.\n",
    "\n",
    "We then calculate the mean of each image and substract each row by that value.\n",
    "\n",
    "We then try to reduce the number of feutres by projecting on a principal subspace, with the PCA algorithm. To do so we first run it on the data, then print the variances and decide the number of features we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardization of the data\n",
    "X = np.divide(X, 127).astype(\"float64\")\n",
    "X -= 1\n",
    "\n",
    "# calculate the mean of each image\n",
    "mean = np.mean(X, axis=1) # shape (50000,)\n",
    "X = (X.transpose() - mean) # substract each row by corresponding mean\n",
    "X = X.transpose() # transpose X back\n",
    "\n",
    "# apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< e^-10: 1\n",
      "< e^-9: 1\n",
      "< e^-8: 1\n",
      "< e^-7: 1\n",
      "< e^-6: 1\n",
      "< e^-5: 37\n",
      "< e^-4: 584\n",
      "< e^-3: 1365\n",
      "< e^-2: 2141\n",
      "< e^-1: 2759\n",
      "< e^0: 3007\n",
      "< e^1: 3062\n",
      "< e^2: 3071\n",
      "< e^3: 3072\n",
      "< e^4: 3072\n",
      "< e^5: 3072\n",
      "< e^6: 3072\n",
      "< e^7: 3072\n",
      "< e^8: 3072\n",
      "< e^9: 3072\n"
     ]
    }
   ],
   "source": [
    "for i in range(-10,10):\n",
    "    a = 0\n",
    "    for x in pca.explained_variance_:\n",
    "        if x < pow(10,i):\n",
    "            a = a + 1\n",
    "    print(\"< e^\"+str(i)+\": \"+str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to keep 2400 features, thus disregarding the ~600 features with a covariance inferiour to 10^-4. This will already help a lot with the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.n_components = 2400\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced.shape\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_reduced.npy\", X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reduced = np.load(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_reduced.npy\")\n",
    "y = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.16218563e+01,   4.35054736e+00,  -1.06141394e+01, ...,\n",
       "          4.35081176e-03,   1.32960065e-03,   1.23443474e-03],\n",
       "       [  7.61379453e-01,   1.16192626e+01,   1.71488161e+00, ...,\n",
       "         -2.13114500e-03,   2.52875689e-03,  -6.36138839e-03],\n",
       "       [ -2.76356677e+01,   5.13644401e-01,  -1.40725825e+00, ...,\n",
       "         -1.36695424e-03,  -1.30393012e-03,  -7.70575473e-03],\n",
       "       ..., \n",
       "       [  8.69513291e+00,  -6.43107970e+00,  -2.49142396e+00, ...,\n",
       "         -2.55107013e-03,   1.80564627e-03,  -4.57465015e-03],\n",
       "       [ -6.48995440e-01,  -8.32589757e-01,   1.58659106e+01, ...,\n",
       "         -8.67655850e-04,  -5.13843359e-03,   2.22656382e-02],\n",
       "       [ -1.20069510e+01,  -4.86847553e+00,  -1.58816572e+00, ...,\n",
       "         -3.69728451e-04,  -2.61162974e-03,   1.34713022e-04]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train our 10 different logistic regressions, using the one vs all method (one class at 1, the others at 0), and collect our 10 classifiers in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def classifier_onevsall(data, labels, num_class):\n",
    "    labels_onevsall = (labels == num_class).astype(int)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(data, labels_onevsall)\n",
    "    return logreg\n",
    "\n",
    "classifiers = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    classifiers.append(classifier_onevsall(X_reduced, y, i))\n",
    "\n",
    "print(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now evaluate our models on the training data and test data. We hence first need to import and process the test data, we'll write a f'n for speeding up the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_data(data):\n",
    "    # get values between -1 and 1\n",
    "    data = np.divide(data, 127).astype(\"float64\")\n",
    "    data -= 1\n",
    "    \n",
    "    # calculate the mean of each image\n",
    "    mean = np.mean(data, axis=1) # shape (50000,)\n",
    "    data = (data.transpose() - mean) # substract each row by corresponding mean\n",
    "    # note that data is now transposed, shape (3072, 50000), let's put it back\n",
    "    data = data.transpose()\n",
    "    \n",
    "    # apply the PCA algorithm\n",
    "    pca = PCA(n_components=2400)\n",
    "    data = pca.fit_transform(data)\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "X_test = get_data(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\test\\\\\", 10000)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test.npy\", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2400)\n"
     ]
    }
   ],
   "source": [
    "X_test_reduced = preprocessing_data(X_test)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test_reduced.npy\", X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test_reduced.npy\")\n",
    "y_test = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\test\\\\labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the models on our test data, and see how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_onevsall(classifiers, data, nbr_classes=10):\n",
    "    predictions = np.empty((data.shape[0],nbr_classes))\n",
    "    for i in range(nbr_classes):\n",
    "        predictor = classifiers[i]\n",
    "        # put the predicted values by each classifier for the whole data (shape (nbr_elements,nbr_classes))\n",
    "        predictions[:, i] = predictor.predict(data)\n",
    "    # return the indice of highest element (so where 1 is predicted)\n",
    "    # if more than one classifier returned 1 for that sample, first encountered is kept\n",
    "    print(predictions.shape)\n",
    "    pred_indices = np.argmax(predictions, axis=1)\n",
    "    return pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_predictions = test_onevsall(classifiers, X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "test_predictions = test_onevsall(classifiers, X_test_reduced, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2683\n",
      "0.115\n"
     ]
    }
   ],
   "source": [
    "# now that we have our predictions, we'll calculate the percentage of right answers\n",
    "print(np.mean(y == train_predictions))\n",
    "print(np.mean(y_test == test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On our train data we do fairly well (26,8%), but on the test data we have very bad results, only 11,5% which is only slightly better than just choosing the same class everytime (10%).\n",
    "\n",
    "We'll hence try a more powerful model and build a Softmax regression.\n",
    "\n",
    "We'll now create a function to create layers of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X,W) + b\n",
    "        \n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now proceed to create the layers\n",
    "\n",
    "First we'll create 1 layer and analyze it's output\n",
    "\n",
    "Notice that we'll already leave the 2nd hidden layer already setted.\n",
    "Also, a good way to choose the number of neurons on each layer is to remember a funnel in order to narrow and filter through each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2400])\n",
    "Y = tf.placeholder(tf.float32, None)\n",
    "n_inputs = 2400\n",
    "n_hidden1 = 1000\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(10,) dtype=float64_ref>\", \"<tf.Variable 'Variable_2:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_7:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_8:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_9:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_10:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_11:0' shape=(10,) dtype=float32_ref>\"] and loss Tensor(\"Mean:0\", shape=(), dtype=float64).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-729d209e3017>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sentinel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# choose our optimization algo (gradient descent) minimizing our cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    320\u001b[0m           \u001b[1;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m           \u001b[1;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(10,) dtype=float64_ref>\", \"<tf.Variable 'Variable_2:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_7:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_8:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_9:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_10:0' shape=(2400, 10) dtype=float32_ref>\", \"<tf.Variable 'Variable_11:0' shape=(10,) dtype=float32_ref>\"] and loss Tensor(\"Mean:0\", shape=(), dtype=float64)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# placeholder for our data, shape (nbr_samples, 2400)\n",
    "x = tf.placeholder(tf.float32, [None, 2400])\n",
    "# our weights W (2400, 10) and biases b for each class (10)\n",
    "W = tf.Variable(tf.zeros([2400, 10]), tf.float32)\n",
    "b = tf.Variable(tf.zeros([10], tf.float32))\n",
    "# executing Wx + b \n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# now to define our cost f'n (cross entropy) and do softmax, then get the computed losses\n",
    "total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=y, logits=y_test))\n",
    "# choose our optimization algo (gradient descent) minimizing our cost\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
