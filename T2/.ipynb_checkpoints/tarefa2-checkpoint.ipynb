{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instructions\n",
    "* One vs all logistic regression\n",
    "* Softmax regression = generalization to handle multiple classes\n",
    "* Neural network with one hidden layer, and numerically checking the gradient\n",
    "* Now 2 hidden layers and different activation f'ns, see what performs best\n",
    "* With best model, do confusion matrix on test set\n",
    "* 4 page report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notebook setup\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Importing the data\n",
    "\n",
    "We have 50 000 32x32 images in the train set and a \"labels\" file with 50 000 lines.\n",
    "\n",
    "For the images, we'll store them as a 50000x3072 np array, so the first 1024 columns are the value of the red pixel in the image of that row, and the next two 1024 columns are the green and blue values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(cifar_dirname=\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\\", upperbound=50000):    \n",
    "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    X = np.empty(shape=(1,3072))\n",
    "    # we load the 50 000 images in X one after the other, one image being 1x3072, to obtain the 50000x3072 array\n",
    "    for i in range(upperbound):\n",
    "        # to have an update of where we're at once in a while\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        # open the current image\n",
    "        current_im  = Image.open(cifar_dirname + str(i).zfill(5) + \".png\" )\n",
    "        # reshape it into an (1,3072) array, so the red values of all the pixels, then green then blue (32*32*3)\n",
    "        reshaped_im = np.reshape(np.asarray(current_im, 'uint8'), (1,3072))\n",
    "        # vertical stack the image into X, what will contain all the images\n",
    "        X           = np.vstack([X, reshaped_im])\n",
    "        \n",
    "    # we want the array to be of type unsigned int on 8 bit (between 0 and 255), so that it occupies the minimum space\n",
    "    X = X.astype(\"uint8\")\n",
    "    X = np.delete(X, (0), axis=0) # delete the first row that's empty\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = get_data()\n",
    "# we load the 50 000 labels for the txt file\n",
    "y = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\labels\")\n",
    "y.shape\n",
    "# since loading the images took a lot of time (~3hr), we'll save them in a binary file (.npy)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now to get the nparray back\n",
    "X = np.load(\"../T2/images.npy\")\n",
    "# also, for the y array, like earlier:\n",
    "y = np.loadtxt(\"../T2/train/labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One vs all classifiers\n",
    "\n",
    "We need to do feature scaling on the images before feeding them into the algorithms. We first get the values between -1 and 1: since they're all between 0 and 255, we'll divide by 127 and substract 1.\n",
    "\n",
    "We then calculate the mean of each image and substract each row by that value.\n",
    "\n",
    "We then try to reduce the number of feutres by projecting on a principal subspace, with the PCA algorithm. To do so we first run it on the data, then print the variances and decide the number of features we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardization of the data\n",
    "X = np.divide(X, 127).astype(\"float64\")\n",
    "X -= 1\n",
    "\n",
    "# calculate the mean of each image\n",
    "mean = np.mean(X, axis=1) # shape (50000,)\n",
    "X = (X.transpose() - mean) # substract each row by corresponding mean\n",
    "X = X.transpose() # transpose X back\n",
    "\n",
    "# apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< e^-10: 1\n",
      "< e^-9: 1\n",
      "< e^-8: 1\n",
      "< e^-7: 1\n",
      "< e^-6: 1\n",
      "< e^-5: 37\n",
      "< e^-4: 584\n",
      "< e^-3: 1365\n",
      "< e^-2: 2141\n",
      "< e^-1: 2759\n",
      "< e^0: 3007\n",
      "< e^1: 3062\n",
      "< e^2: 3071\n",
      "< e^3: 3072\n",
      "< e^4: 3072\n",
      "< e^5: 3072\n",
      "< e^6: 3072\n",
      "< e^7: 3072\n",
      "< e^8: 3072\n",
      "< e^9: 3072\n"
     ]
    }
   ],
   "source": [
    "for i in range(-10,10):\n",
    "    a = 0\n",
    "    for x in pca.explained_variance_:\n",
    "        if x < pow(10,i):\n",
    "            a = a + 1\n",
    "    print(\"< e^\"+str(i)+\": \"+str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to keep 2400 features, thus disregarding the ~600 features with a covariance inferiour to 10^-4. This will already help a lot with the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.n_components = 2400\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced.shape\n",
    "np.save(\"../T2/images_reduced.npy\", X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reduced = np.load(\"../T2/images_reduced.npy\")\n",
    "y = np.loadtxt(\"../T2/train/labels\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.16218563e+01,   4.35054736e+00,  -1.06141394e+01, ...,\n",
       "         -3.85243468e-03,   1.49823087e-02,   1.18771391e-02],\n",
       "       [  7.61379453e-01,   1.16192626e+01,   1.71488161e+00, ...,\n",
       "         -4.27453536e-03,  -1.07817977e-02,  -2.84231972e-03],\n",
       "       [ -2.76356677e+01,   5.13644401e-01,  -1.40725825e+00, ...,\n",
       "         -5.98168370e-03,  -8.11071692e-03,   3.79169774e-03],\n",
       "       ..., \n",
       "       [  8.69513291e+00,  -6.43107970e+00,  -2.49142396e+00, ...,\n",
       "          6.53476378e-04,  -5.13104828e-04,  -1.07622296e-02],\n",
       "       [ -6.48995440e-01,  -8.32589757e-01,   1.58659106e+01, ...,\n",
       "          8.83920684e-03,  -1.50775800e-02,   1.29620180e-02],\n",
       "       [ -1.20069510e+01,  -4.86847553e+00,  -1.58816572e+00, ...,\n",
       "         -2.84157815e-03,   7.87094837e-03,   8.05116643e-04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train our 10 different logistic regressions, using the one vs all method (one class at 1, the others at 0), and collect our 10 classifiers in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def classifier_onevsall(data, labels, num_class):\n",
    "    labels_onevsall = (labels == num_class).astype(int)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(data, labels_onevsall)\n",
    "    return logreg\n",
    "\n",
    "classifiers = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    classifiers.append(classifier_onevsall(X_reduced, y, i))\n",
    "\n",
    "print(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now evaluate our models on the training data and test data. We hence first need to import and process the test data, we'll write a f'n for speeding up the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_data(data):\n",
    "    # get values between -1 and 1\n",
    "    data = np.divide(data, 127).astype(\"float64\")\n",
    "    data -= 1\n",
    "    \n",
    "    # calculate the mean of each image\n",
    "    mean = np.mean(data, axis=1) # shape (50000,)\n",
    "    data = (data.transpose() - mean) # substract each row by corresponding mean\n",
    "    # note that data is now transposed, shape (3072, 50000), let's put it back\n",
    "    data = data.transpose()\n",
    "    \n",
    "    # apply the PCA algorithm\n",
    "    pca = PCA(n_components=2400)\n",
    "    data = pca.fit_transform(data)\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "X_test = get_data(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\test\\\\\", 10000)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test.npy\", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2400)\n"
     ]
    }
   ],
   "source": [
    "X_test_reduced = preprocessing_data(X_test)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test_reduced.npy\", X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"../T2/images_test_reduced.npy\")\n",
    "y_test = np.loadtxt(\"../T2/test/labels\", dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the models on our test data, and see how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_onevsall(classifiers, data, nbr_classes=10):\n",
    "    predictions = np.empty((data.shape[0],nbr_classes))\n",
    "    for i in range(nbr_classes):\n",
    "        predictor = classifiers[i]\n",
    "        # put the predicted values by each classifier for the whole data (shape (nbr_elements,nbr_classes))\n",
    "        predictions[:, i] = predictor.predict(data)\n",
    "    # return the indice of highest element (so where 1 is predicted)\n",
    "    # if more than one classifier returned 1 for that sample, first encountered is kept\n",
    "    print(predictions.shape)\n",
    "    pred_indices = np.argmax(predictions, axis=1)\n",
    "    return pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_predictions = test_onevsall(classifiers, X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "test_predictions = test_onevsall(classifiers, X_test_reduced, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2683\n",
      "0.115\n"
     ]
    }
   ],
   "source": [
    "# now that we have our predictions, we'll calculate the percentage of right answers\n",
    "print(np.mean(y == train_predictions))\n",
    "print(np.mean(y_test == test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On our train data we do fairly well (26,8%), but on the test data we have very bad results, only 11,5% which is only slightly better than just choosing the same class everytime (10%).\n",
    "\n",
    "We'll hence try a more powerful model and build a Softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxRegression(object):\n",
    "\n",
    "    \"\"\"Softmax regression classifier.\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float (default: 0.01)\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    epochs : int (default: 50)\n",
    "        Passes over the training dataset.\n",
    "        Prior to each epoch, the dataset is shuffled\n",
    "        if `minibatches > 1` to prevent cycles in stochastic gradient descent.\n",
    "    l2 : float\n",
    "        Regularization parameter for L2 regularization.\n",
    "        No regularization if l2=0.0.\n",
    "    minibatches : int (default: 1)\n",
    "        The number of minibatches for gradient-based optimization.\n",
    "        If 1: Gradient Descent learning\n",
    "        If len(y): Stochastic Gradient Descent (SGD) online learning\n",
    "        If 1 < minibatches < len(y): SGD Minibatch learning\n",
    "    n_classes : int (default: None)\n",
    "        A positive integer to declare the number of class labels\n",
    "        if not all class labels are present in a partial training set.\n",
    "        Gets the number of class labels automatically if None.\n",
    "    random_seed : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 2d-array, shape={n_features, 1}\n",
    "      Model weights after fitting.\n",
    "    b_ : 1d-array, shape={1,}\n",
    "      Bias unit after fitting.\n",
    "    cost_ : list\n",
    "        List of floats, the average cross_entropy for each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, epochs=50,\n",
    "                 l2=0.0,\n",
    "                 minibatches=1,\n",
    "                 n_classes=None,\n",
    "                 random_seed=None):\n",
    "\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.minibatches = minibatches\n",
    "        self.n_classes = n_classes\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def _fit(self, X, y, init_params=True):\n",
    "        if init_params:\n",
    "            if self.n_classes is None:\n",
    "                self.n_classes = np.max(y) + 1\n",
    "            self._n_features = X.shape[1]\n",
    "\n",
    "            self.b_, self.w_ = self._init_params(\n",
    "                weights_shape=(self._n_features, self.n_classes),\n",
    "                bias_shape=(self.n_classes,),\n",
    "                random_seed=self.random_seed)\n",
    "            self.cost_ = []\n",
    "\n",
    "        y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for idx in self._yield_minibatches_idx(\n",
    "                    n_batches=self.minibatches,\n",
    "                    data_ary=y,\n",
    "                    shuffle=True):\n",
    "                # givens:\n",
    "                # w_ -> n_feat x n_classes\n",
    "                # b_  -> n_classes\n",
    "\n",
    "                # net_input, softmax and diff -> n_samples x n_classes:\n",
    "                net = self._net_input(X[idx], self.w_, self.b_)\n",
    "                softm = self._softmax(net)\n",
    "                diff = softm - y_enc[idx]\n",
    "                mse = np.mean(diff, axis=0)\n",
    "\n",
    "                # gradient -> n_features x n_classes\n",
    "                grad = np.dot(X[idx].T, diff)\n",
    "                \n",
    "                # update in opp. direction of the cost gradient\n",
    "                self.w_ -= (self.eta * grad +\n",
    "                            self.eta * self.l2 * self.w_)\n",
    "                self.b_ -= (self.eta * np.sum(diff, axis=0))\n",
    "\n",
    "            # compute cost of the whole epoch\n",
    "            net = self._net_input(X, self.w_, self.b_)\n",
    "            softm = self._softmax(net)\n",
    "            cross_ent = self._cross_entropy(output=softm, y_target=y_enc)\n",
    "            cost = self._cost(cross_ent)\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y, init_params=True):\n",
    "        \"\"\"Learn model from training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        init_params : bool (default: True)\n",
    "            Re-initializes model parametersprior to fitting.\n",
    "            Set False to continue training with weights from\n",
    "            a previous model fitting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        if self.random_seed is not None:\n",
    "            np.random.seed(self.random_seed)\n",
    "        self._fit(X=X, y=y, init_params=init_params)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self._to_classlabels(probas)\n",
    " \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict targets from X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        target_values : array-like, shape = [n_samples]\n",
    "          Predicted target values.\n",
    "\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise AttributeError('Model is not fitted, yet.')\n",
    "        return self._predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of X from the net input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Class probabilties : array-like, shape= [n_samples, n_classes]\n",
    "\n",
    "        \"\"\"\n",
    "        net = self._net_input(X, self.w_, self.b_)\n",
    "        softm = self._softmax(net)\n",
    "        return softm\n",
    "\n",
    "    def _net_input(self, X, W, b):\n",
    "        return (X.dot(W) + b)\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def _cross_entropy(self, output, y_target):\n",
    "        return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "    def _cost(self, cross_entropy):\n",
    "        L2_term = self.l2 * np.sum(self.w_ ** 2)\n",
    "        cross_entropy = cross_entropy + L2_term\n",
    "        return 0.5 * np.mean(cross_entropy)\n",
    "\n",
    "    def _to_classlabels(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "    \n",
    "    def _init_params(self, weights_shape, bias_shape=(1,), dtype='float64',\n",
    "                     scale=0.01, random_seed=None):\n",
    "        \"\"\"Initialize weight coefficients.\"\"\"\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "        w = np.random.normal(loc=0.0, scale=scale, size=weights_shape)\n",
    "        b = np.zeros(shape=bias_shape)\n",
    "        return b.astype(dtype), w.astype(dtype)\n",
    "    \n",
    "    def _one_hot(self, y, n_labels, dtype):\n",
    "        \"\"\"Returns a matrix where each sample in y is represented\n",
    "           as a row, and each column represents the class label in\n",
    "           the one-hot encoding scheme.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            y = np.array([0, 1, 2, 3, 4, 2])\n",
    "            mc = _BaseMultiClass()\n",
    "            mc._one_hot(y=y, n_labels=5, dtype='float')\n",
    "\n",
    "            np.array([[1., 0., 0., 0., 0.],\n",
    "                      [0., 1., 0., 0., 0.],\n",
    "                      [0., 0., 1., 0., 0.],\n",
    "                      [0., 0., 0., 1., 0.],\n",
    "                      [0., 0., 0., 0., 1.],\n",
    "                      [0., 0., 1., 0., 0.]])\n",
    "\n",
    "        \"\"\"\n",
    "        mat = np.zeros((len(y), n_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            mat[i, val] = 1\n",
    "        return mat.astype(dtype)    \n",
    "    \n",
    "    def _yield_minibatches_idx(self, n_batches, data_ary, shuffle=True):\n",
    "            indices = np.arange(data_ary.shape[0])\n",
    "\n",
    "            if shuffle:\n",
    "                indices = np.random.permutation(indices)\n",
    "            if n_batches > 1:\n",
    "                remainder = data_ary.shape[0] % n_batches\n",
    "\n",
    "                if remainder:\n",
    "                    minis = np.array_split(indices[:-remainder], n_batches)\n",
    "                    minis[-1] = np.concatenate((minis[-1],\n",
    "                                                indices[-remainder:]),\n",
    "                                               axis=0)\n",
    "                else:\n",
    "                    minis = np.array_split(indices, n_batches)\n",
    "\n",
    "            else:\n",
    "                minis = (indices,)\n",
    "\n",
    "            for idx_batch in minis:\n",
    "                yield idx_batch\n",
    "    \n",
    "    def _shuffle_arrays(self, arrays):\n",
    "        \"\"\"Shuffle arrays in unison.\"\"\"\n",
    "        r = np.random.permutation(len(arrays[0]))\n",
    "        return [ary[r] for ary in arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "softmax = SoftmaxRegression(eta=0.01, epochs=10, minibatches=10, random_seed=42)\n",
    "softmax.fit(X_reduced, y)\n",
    "Y_soft_pred = softmax.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_test == Y_soft_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a neural network with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpxwfitaxd\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015B71A12E10>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\VICTOR~1\\\\AppData\\\\Local\\\\Temp\\\\tmpxwfitaxd'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpxwfitaxd\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.32078, step = 1\n",
      "INFO:tensorflow:global_step/sec: 5.7894\n",
      "INFO:tensorflow:loss = 2.09553, step = 101 (17.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.65578\n",
      "INFO:tensorflow:loss = 2.22212, step = 201 (17.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.79849\n",
      "INFO:tensorflow:loss = 1.59118, step = 301 (17.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.79512\n",
      "INFO:tensorflow:loss = 1.57916, step = 401 (17.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.83104\n",
      "INFO:tensorflow:loss = 1.69891, step = 501 (17.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.83923\n",
      "INFO:tensorflow:loss = 1.87035, step = 601 (17.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.8137\n",
      "INFO:tensorflow:loss = 1.72836, step = 701 (17.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.72493\n",
      "INFO:tensorflow:loss = 1.41268, step = 801 (17.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.46742\n",
      "INFO:tensorflow:loss = 1.53861, step = 901 (18.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.80524\n",
      "INFO:tensorflow:loss = 1.53232, step = 1001 (17.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.6433\n",
      "INFO:tensorflow:loss = 1.35706, step = 1101 (17.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95676\n",
      "INFO:tensorflow:loss = 1.29089, step = 1201 (16.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.28682\n",
      "INFO:tensorflow:loss = 1.30321, step = 1301 (15.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2291\n",
      "INFO:tensorflow:loss = 1.32081, step = 1401 (16.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26234\n",
      "INFO:tensorflow:loss = 1.30133, step = 1501 (15.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.27534\n",
      "INFO:tensorflow:loss = 1.35878, step = 1601 (15.931 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26588\n",
      "INFO:tensorflow:loss = 1.49355, step = 1701 (15.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26509\n",
      "INFO:tensorflow:loss = 1.34397, step = 1801 (15.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.30151\n",
      "INFO:tensorflow:loss = 1.1817, step = 1901 (15.870 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.28009\n",
      "INFO:tensorflow:loss = 0.996324, step = 2001 (15.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.25057\n",
      "INFO:tensorflow:loss = 1.04353, step = 2101 (16.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.24118\n",
      "INFO:tensorflow:loss = 1.10642, step = 2201 (16.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.38502\n",
      "INFO:tensorflow:loss = 0.956883, step = 2301 (15.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.23962\n",
      "INFO:tensorflow:loss = 1.27943, step = 2401 (16.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.31788\n",
      "INFO:tensorflow:loss = 1.25218, step = 2501 (15.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.344\n",
      "INFO:tensorflow:loss = 1.26063, step = 2601 (15.763 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35006\n",
      "INFO:tensorflow:loss = 0.954234, step = 2701 (15.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.65015\n",
      "INFO:tensorflow:loss = 1.17321, step = 2801 (17.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.87604\n",
      "INFO:tensorflow:loss = 0.908044, step = 2901 (17.021 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.92245\n",
      "INFO:tensorflow:loss = 0.822325, step = 3001 (16.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.02477\n",
      "INFO:tensorflow:loss = 0.813728, step = 3101 (16.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.31948\n",
      "INFO:tensorflow:loss = 0.786114, step = 3201 (15.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3047\n",
      "INFO:tensorflow:loss = 0.784308, step = 3301 (15.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.27574\n",
      "INFO:tensorflow:loss = 1.09576, step = 3401 (15.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.29555\n",
      "INFO:tensorflow:loss = 0.88959, step = 3501 (15.882 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.23533\n",
      "INFO:tensorflow:loss = 0.903002, step = 3601 (16.039 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3629 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpxwfitaxd\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 6.0313\n",
      "INFO:tensorflow:loss = 0.991272, step = 3701 (16.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.32549\n",
      "INFO:tensorflow:loss = 1.12767, step = 3801 (15.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.83666\n",
      "INFO:tensorflow:loss = 1.06464, step = 3901 (17.135 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpxwfitaxd\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.838572.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_reduced)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[5000], n_classes=10, feature_columns=feature_cols, config=config)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1\n",
    "dnn_clf.fit(X_reduced, y, batch_size=50, steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpxwfitaxd\\model.ckpt-4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1807"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4715865068787259"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_pred_proba = y_pred['probabilities']\n",
    "log_loss(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the training with 2 hidden layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpshwqaiv5\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015B39634A90>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\VICTOR~1\\\\AppData\\\\Local\\\\Temp\\\\tmpshwqaiv5'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Users\\VictorAkiraHassudaSi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpshwqaiv5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.26992, step = 1\n",
      "INFO:tensorflow:global_step/sec: 4.16079\n",
      "INFO:tensorflow:loss = 1.82596, step = 101 (24.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.2034\n",
      "INFO:tensorflow:loss = 1.88551, step = 201 (23.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.20748\n",
      "INFO:tensorflow:loss = 1.54211, step = 301 (23.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.18488\n",
      "INFO:tensorflow:loss = 1.51872, step = 401 (23.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.18242\n",
      "INFO:tensorflow:loss = 1.41487, step = 501 (23.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.16635\n",
      "INFO:tensorflow:loss = 1.67424, step = 601 (23.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.62195\n",
      "INFO:tensorflow:loss = 1.64547, step = 701 (27.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.77488\n",
      "INFO:tensorflow:loss = 1.27305, step = 801 (26.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.92649\n",
      "INFO:tensorflow:loss = 1.46385, step = 901 (25.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.23301\n",
      "INFO:tensorflow:loss = 1.51239, step = 1001 (23.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.20375\n",
      "INFO:tensorflow:loss = 1.3931, step = 1101 (23.768 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.43894\n",
      "INFO:tensorflow:loss = 1.22213, step = 1201 (22.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.5408\n",
      "INFO:tensorflow:loss = 1.15069, step = 1301 (22.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.53048\n",
      "INFO:tensorflow:loss = 1.26437, step = 1401 (22.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.53646\n",
      "INFO:tensorflow:loss = 1.3077, step = 1501 (22.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.51183\n",
      "INFO:tensorflow:loss = 1.25974, step = 1601 (22.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.51592\n",
      "INFO:tensorflow:loss = 1.4746, step = 1701 (22.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.44171\n",
      "INFO:tensorflow:loss = 1.3336, step = 1801 (22.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.52514\n",
      "INFO:tensorflow:loss = 1.23334, step = 1901 (22.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.29962\n",
      "INFO:tensorflow:loss = 0.959645, step = 2001 (23.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.23427\n",
      "INFO:tensorflow:loss = 1.0508, step = 2101 (23.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.69805\n",
      "INFO:tensorflow:loss = 1.01557, step = 2201 (27.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.94907\n",
      "INFO:tensorflow:loss = 0.924348, step = 2301 (25.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.9377\n",
      "INFO:tensorflow:loss = 1.1486, step = 2401 (25.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.21584\n",
      "INFO:tensorflow:loss = 1.17164, step = 2501 (23.721 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2511 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpshwqaiv5\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.92091\n",
      "INFO:tensorflow:loss = 1.20221, step = 2601 (25.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.20482\n",
      "INFO:tensorflow:loss = 0.949531, step = 2701 (23.783 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.14608\n",
      "INFO:tensorflow:loss = 0.941211, step = 2801 (24.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.12926\n",
      "INFO:tensorflow:loss = 0.885807, step = 2901 (24.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.20375\n",
      "INFO:tensorflow:loss = 0.804672, step = 3001 (23.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.19544\n",
      "INFO:tensorflow:loss = 0.930522, step = 3101 (23.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.19509\n",
      "INFO:tensorflow:loss = 0.83209, step = 3201 (23.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.23104\n",
      "INFO:tensorflow:loss = 0.84558, step = 3301 (23.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.20057\n",
      "INFO:tensorflow:loss = 1.05203, step = 3401 (23.803 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.01584\n",
      "INFO:tensorflow:loss = 0.932929, step = 3501 (24.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.81669\n",
      "INFO:tensorflow:loss = 0.87959, step = 3601 (26.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.21263\n",
      "INFO:tensorflow:loss = 0.862775, step = 3701 (23.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.20039\n",
      "INFO:tensorflow:loss = 0.965202, step = 3801 (23.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.22333\n",
      "INFO:tensorflow:loss = 1.07098, step = 3901 (23.663 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpshwqaiv5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.930539.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[5000,1000], n_classes=10, feature_columns=feature_cols, config=config)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1\n",
    "dnn_clf.fit(X_reduced, y, batch_size=50, steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\VICTOR~1\\AppData\\Local\\Temp\\tmpshwqaiv5\\model.ckpt-4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17169999999999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred2 = dnn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred2['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2922673461669825"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_pred_proba2 = y_pred2['probabilities']\n",
    "log_loss(y_test, y_pred_proba2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
