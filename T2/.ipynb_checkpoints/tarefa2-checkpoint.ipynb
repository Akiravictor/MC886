{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instructions\n",
    "* One vs all logistic regression\n",
    "* Softmax regression = generalization to handle multiple classes\n",
    "* Neural network with one hidden layer, and numerically checking the gradient\n",
    "* Now 2 hidden layers and different activation f'ns, see what performs best\n",
    "* With best model, do confusion matrix on test set\n",
    "* 4 page report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notebook setup\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Importing the data\n",
    "\n",
    "We have 50 000 32x32 images in the train set and a \"labels\" file with 50 000 lines.\n",
    "\n",
    "For the images, we'll store them as a 50000x3072 np array, so the first 1024 columns are the value of the red pixel in the image of that row, and the next two 1024 columns are the green and blue values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(cifar_dirname=\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\\", upperbound=50000):    \n",
    "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    X = np.empty(shape=(1,3072))\n",
    "    # we load the 50 000 images in X one after the other, one image being 1x3072, to obtain the 50000x3072 array\n",
    "    for i in range(upperbound):\n",
    "        # to have an update of where we're at once in a while\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        # open the current image\n",
    "        current_im  = Image.open(cifar_dirname + str(i).zfill(5) + \".png\" )\n",
    "        # reshape it into an (1,3072) array, so the red values of all the pixels, then green then blue (32*32*3)\n",
    "        reshaped_im = np.reshape(np.asarray(current_im, 'uint8'), (1,3072))\n",
    "        # vertical stack the image into X, what will contain all the images\n",
    "        X           = np.vstack([X, reshaped_im])\n",
    "        \n",
    "    # we want the array to be of type unsigned int on 8 bit (between 0 and 255), so that it occupies the minimum space\n",
    "    X = X.astype(\"uint8\")\n",
    "    X = np.delete(X, (0), axis=0) # delete the first row that's empty\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = get_data()\n",
    "# we load the 50 000 labels for the txt file\n",
    "y = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\labels\")\n",
    "y.shape\n",
    "# since loading the images took a lot of time (~3hr), we'll save them in a binary file (.npy)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now to get the nparray back\n",
    "X = np.load(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images.npy\")\n",
    "# also, for the y array, like earlier:\n",
    "y = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One vs all classifiers\n",
    "\n",
    "We need to do feature scaling on the images before feeding them into the algorithms. We first get the values between -1 and 1: since they're all between 0 and 255, we'll divide by 127 and substract 1.\n",
    "\n",
    "We then calculate the mean of each image and substract each row by that value.\n",
    "\n",
    "We then try to reduce the number of feutres by projecting on a principal subspace, with the PCA algorithm. To do so we first run it on the data, then print the variances and decide the number of features we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardization of the data\n",
    "X = np.divide(X, 127).astype(\"float64\")\n",
    "X -= 1\n",
    "\n",
    "# calculate the mean of each image\n",
    "mean = np.mean(X, axis=1) # shape (50000,)\n",
    "X = (X.transpose() - mean) # substract each row by corresponding mean\n",
    "X = X.transpose() # transpose X back\n",
    "\n",
    "# apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< e^-10: 1\n",
      "< e^-9: 1\n",
      "< e^-8: 1\n",
      "< e^-7: 1\n",
      "< e^-6: 1\n",
      "< e^-5: 37\n",
      "< e^-4: 584\n",
      "< e^-3: 1365\n",
      "< e^-2: 2141\n",
      "< e^-1: 2759\n",
      "< e^0: 3007\n",
      "< e^1: 3062\n",
      "< e^2: 3071\n",
      "< e^3: 3072\n",
      "< e^4: 3072\n",
      "< e^5: 3072\n",
      "< e^6: 3072\n",
      "< e^7: 3072\n",
      "< e^8: 3072\n",
      "< e^9: 3072\n"
     ]
    }
   ],
   "source": [
    "for i in range(-10,10):\n",
    "    a = 0\n",
    "    for x in pca.explained_variance_:\n",
    "        if x < pow(10,i):\n",
    "            a = a + 1\n",
    "    print(\"< e^\"+str(i)+\": \"+str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to keep 2400 features, thus disregarding the ~600 features with a covariance inferiour to 10^-4. This will already help a lot with the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.n_components = 2400\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced.shape\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_reduced.npy\", X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reduced = np.load(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_reduced.npy\")\n",
    "y = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\train\\\\labels\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.16218563e+01,   4.35054736e+00,  -1.06141394e+01, ...,\n",
       "          4.35081176e-03,   1.32960065e-03,   1.23443474e-03],\n",
       "       [  7.61379453e-01,   1.16192626e+01,   1.71488161e+00, ...,\n",
       "         -2.13114500e-03,   2.52875689e-03,  -6.36138839e-03],\n",
       "       [ -2.76356677e+01,   5.13644401e-01,  -1.40725825e+00, ...,\n",
       "         -1.36695424e-03,  -1.30393012e-03,  -7.70575473e-03],\n",
       "       ..., \n",
       "       [  8.69513291e+00,  -6.43107970e+00,  -2.49142396e+00, ...,\n",
       "         -2.55107013e-03,   1.80564627e-03,  -4.57465015e-03],\n",
       "       [ -6.48995440e-01,  -8.32589757e-01,   1.58659106e+01, ...,\n",
       "         -8.67655850e-04,  -5.13843359e-03,   2.22656382e-02],\n",
       "       [ -1.20069510e+01,  -4.86847553e+00,  -1.58816572e+00, ...,\n",
       "         -3.69728451e-04,  -2.61162974e-03,   1.34713022e-04]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train our 10 different logistic regressions, using the one vs all method (one class at 1, the others at 0), and collect our 10 classifiers in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def classifier_onevsall(data, labels, num_class):\n",
    "    labels_onevsall = (labels == num_class).astype(int)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(data, labels_onevsall)\n",
    "    return logreg\n",
    "\n",
    "classifiers = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    classifiers.append(classifier_onevsall(X_reduced, y, i))\n",
    "\n",
    "print(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now evaluate our models on the training data and test data. We hence first need to import and process the test data, we'll write a f'n for speeding up the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_data(data):\n",
    "    # get values between -1 and 1\n",
    "    data = np.divide(data, 127).astype(\"float64\")\n",
    "    data -= 1\n",
    "    \n",
    "    # calculate the mean of each image\n",
    "    mean = np.mean(data, axis=1) # shape (50000,)\n",
    "    data = (data.transpose() - mean) # substract each row by corresponding mean\n",
    "    # note that data is now transposed, shape (3072, 50000), let's put it back\n",
    "    data = data.transpose()\n",
    "    \n",
    "    # apply the PCA algorithm\n",
    "    pca = PCA(n_components=2400)\n",
    "    data = pca.fit_transform(data)\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "X_test = get_data(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\test\\\\\", 10000)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test.npy\", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2400)\n"
     ]
    }
   ],
   "source": [
    "X_test_reduced = preprocessing_data(X_test)\n",
    "np.save(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test_reduced.npy\", X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\images_test_reduced.npy\")\n",
    "y_test = np.loadtxt(\"D:\\\\Unicamp\\\\MC886\\\\Git\\\\T2\\\\test\\\\labels\", dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the models on our test data, and see how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_onevsall(classifiers, data, nbr_classes=10):\n",
    "    predictions = np.empty((data.shape[0],nbr_classes))\n",
    "    for i in range(nbr_classes):\n",
    "        predictor = classifiers[i]\n",
    "        # put the predicted values by each classifier for the whole data (shape (nbr_elements,nbr_classes))\n",
    "        predictions[:, i] = predictor.predict(data)\n",
    "    # return the indice of highest element (so where 1 is predicted)\n",
    "    # if more than one classifier returned 1 for that sample, first encountered is kept\n",
    "    print(predictions.shape)\n",
    "    pred_indices = np.argmax(predictions, axis=1)\n",
    "    return pred_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_predictions = test_onevsall(classifiers, X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "test_predictions = test_onevsall(classifiers, X_test_reduced, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2683\n",
      "0.115\n"
     ]
    }
   ],
   "source": [
    "# now that we have our predictions, we'll calculate the percentage of right answers\n",
    "print(np.mean(y == train_predictions))\n",
    "print(np.mean(y_test == test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On our train data we do fairly well (26,8%), but on the test data we have very bad results, only 11,5% which is only slightly better than just choosing the same class everytime (10%).\n",
    "\n",
    "We'll hence try a more powerful model and build a Softmax regression.\n",
    "\n",
    "We'll now create a function to create layers of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    return importlib.import_module(mname)\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\nImportError: DLL load failed: Não foi possível encontrar o módulo especificado.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: Não foi possível encontrar o módulo especificado.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdlopenflags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_default_dlopen_flags\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e37fd98aa88f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mneuron_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mn_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    return importlib.import_module(mname)\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\nImportError: DLL load failed: Não foi possível encontrar o módulo especificado.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"C:\\Users\\yassine.DESKTOP-NR3SF42\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X,W) + b\n",
    "        \n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now proceed to create the layers\n",
    "\n",
    "First we'll create 1 layer and analyze it's output\n",
    "\n",
    "Notice that we'll already leave the 2nd hidden layer already setted.\n",
    "Also, a good way to choose the number of neurons on each layer is to remember a funnel in order to narrow and filter through each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2400])\n",
    "Y = tf.placeholder(tf.float32, None)\n",
    "n_inputs = 2400\n",
    "n_hidden1 = 1000\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# placeholder for our data, shape (nbr_samples, 2400)\n",
    "x_placeholder = tf.placeholder(tf.float32, [None, 2400])\n",
    "# our weights W (2400, 10) and biases b for each class (10)\n",
    "# we'll not initialize our W with 0, but instead with a truncated normal distribution\n",
    "n_features = 2400\n",
    "n_classes = 10\n",
    "stddev = 2 / np.sqrt(n_features)\n",
    "first_values_W = tf.truncated_normal((n_features, n_classes), stddev=stddev)\n",
    "W = tf.Variable(first_values_W, tf.float32)\n",
    "b = tf.Variable(tf.zeros([10], tf.float32))\n",
    "# executing Wx + b \n",
    "y_hat = tf.matmul(x_placeholder, W) + b\n",
    "# create one hot encoded logits, from the y labels\n",
    "y_onehot = tf.one_hot(y, 10, 1, 0, axis=-1)\n",
    "\n",
    "# now to define our cost f'n (cross entropy) and do softmax, then get the computed losses\n",
    "total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=y_hat))\n",
    "# choose our optimization algo (gradient descent, learning rate 0.5) minimizing our cost\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_steps = 100 # how many times we'll train\n",
    "\n",
    "sess = tf.Session()\n",
    "# initialization\n",
    "sess.run(init) # initialize the variables we defined\n",
    "y_onehot_np = sess.run(y_onehot) # get actual np array from the tensor, to use for training\n",
    "feed_dict = {x_placeholder:X_reduced, y_hat:y_onehot_np} # what we'll feed\n",
    "\n",
    "# training\n",
    "for i in range(n_steps):\n",
    "    sess.run(train_step, feed_dict) # train on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set evaluation: error of 0.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate our model on all samples of the training set\n",
    "_, predicted_indices = tf.nn.top_k(y_hat) # indices with highest value (ie predicted class)\n",
    "predictions = np.array(sess.run([predicted_indices],feed_dict)).reshape((50000,)) # get the result as ndarray (50000,)\n",
    "error = np.mean(predictions != y)\n",
    "\n",
    "print(\"Training set evaluation: error of {}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set evaluation: error of 0.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate our model on all samples of the testing set\n",
    "y_onehot_test = tf.one_hot(y_test, 10, 1, 0, axis=-1)\n",
    "y_onehot_test_np = sess.run(y_onehot)\n",
    "\n",
    "feed_dict_test = {x_placeholder:X_test, y_hat:y_onehot_test_np}\n",
    "predictions = np.array(sess.run([predicted_indices],feed_dict_test)).reshape((50000,)) # get the result as ndarray (50000,)\n",
    "error = np.mean(predictions != y)\n",
    "\n",
    "print(\"Testing set evaluation: error of {}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
